name: Validate Azure OpenAI response

on:
  workflow_dispatch:        # run on demand from the Actions tab

permissions:
  contents: write           # needed to commit and push changes to README.md

jobs:
  run-validation:
    runs-on: ubuntu-latest
    environment: responses   # ðŸ”‘ unlocks the environmentâ€‘scoped secrets

    # Expose the environment secrets as real process envâ€‘vars
    env:
      AZURE_OPENAI_API_KEY:       ${{ secrets.AZURE_OPENAI_API_KEY }}
      AZURE_OPENAI_V1_API_ENDPOINT: ${{ secrets.AZURE_OPENAI_V1_API_ENDPOINT }}
      AZURE_OPENAI_API_MODEL:     ${{ secrets.AZURE_OPENAI_API_MODEL }}

    steps:
    # 1 â€“ check out the repo so the script is available
    - uses: actions/checkout@v4

    # 2 â€“ set up Python
    - uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    # 3 â€“ install the script's two lightweight deps
    - name: Install requirements
      run: |
        python -m pip install --upgrade pip
        pip install openai python-dotenv

    # 4 â€“ run the script, grade the result, assemble a report
    - name: Execute script and capture outcome
      id: test
      shell: bash
      run: |
        set +e                                 # we want to handle failures ourselves
        TIMESTAMP=$(date -u +"%Y-%m-%dT%H:%M:%SZ")

        # === run the user script ===
        python responses-basic-aoai-v1.py > out.txt 2>&1
        EXIT_CODE=$?

        # === decide pass / fail ===
        if [[ $EXIT_CODE -eq 0 && -s out.txt ]]; then
          PASS_FAIL="PASS"
        else
          PASS_FAIL="FAIL"
        fi

        # === build JSON report ===
        jq -n \
          --arg date        "$TIMESTAMP" \
          --arg output      "$(cat out.txt | tr -d '\r')" \
          --arg pass_fail   "$PASS_FAIL" \
          --argjson code    "$EXIT_CODE" \
          '{test_run_date: $date,
            output:        $output,
            pass_fail:     $pass_fail,
            error_code:    $code}' > aoai-test-result.json

    # 5 â€“ update README.md with latest test results
    - name: Update README with test results
      shell: bash
      run: |
        # Read the test results from the previous step
        TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M:%S")
        OUTPUT=$(cat out.txt | tr -d '\r')
        
        # Determine pass/fail based on the exit code from the test step
        if [[ -f aoai-test-result.json ]]; then
          PASS_FAIL=$(jq -r '.pass_fail' aoai-test-result.json)
          ERROR_CODE=$(jq -r '.error_code' aoai-test-result.json)
        else
          PASS_FAIL="FAIL"
          ERROR_CODE="1"
        fi
        
        # Create the new test results section
        echo "# Latest test run results" > test_results_section.md
        echo "" >> test_results_section.md
        echo "This section summarizes the most recent run of the \`test-responses-basic-aoai-v1\` GitHub Action (tests for \`responses-basic-aoai-v1.py\`):" >> test_results_section.md
        echo "" >> test_results_section.md
        echo "- **Date (UTC):** $TIMESTAMP" >> test_results_section.md
        echo "- **Pass/Fail:** $PASS_FAIL" >> test_results_section.md
        echo "- **Error Code:** $ERROR_CODE" >> test_results_section.md
        echo "- **Output:** $(head -n1 out.txt)" >> test_results_section.md
        
        # Add remaining output lines with proper indentation if there are multiple lines
        if [[ $(wc -l < out.txt) -gt 1 ]]; then
          echo "" >> test_results_section.md
          tail -n +2 out.txt | while IFS= read -r line; do
            echo "  $line" >> test_results_section.md
          done
        fi
        
        # Find the line number where "# Latest test run results" starts
        LINE_NUM=$(grep -n "# Latest test run results" README.md | cut -d: -f1)
        
        if [[ -n "$LINE_NUM" ]]; then
          # Remove everything from "# Latest test run results" to the end of file
          head -n $((LINE_NUM - 1)) README.md > README_temp.md
          # Append the new test results section
          cat test_results_section.md >> README_temp.md
          # Replace the original README
          mv README_temp.md README.md
        else
          # If section doesn't exist, append it to the end
          echo "" >> README.md
          cat test_results_section.md >> README.md
        fi
        
        # Clean up temporary file
        rm -f test_results_section.md

    # 6 â€“ commit and push the updated README
    - name: Commit updated README
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add README.md
        if git diff --staged --quiet; then
          echo "No changes to commit"
        else
          git commit -m "Update README with latest test results [skip ci]"
          git push

    # 7 â€“ make the report downloadable from the run summary
    - name: Upload result artifact
      uses: actions/upload-artifact@v4
      with:
        name: aoai-response-test         # folder name visible in the UI
        path: aoai-test-result.json
